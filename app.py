import streamlit as st
from dotenv import load_dotenv
import os
import shutil # python standard library
from PyPDF2 import PdfReader
from langchain.text_splitter import CharacterTextSplitter
import hashlib
# get_conversation_chain(_vectorstore) ì‹¤í–‰ì— í•„ìš”í•œ ëª¨ë“ˆ
# (deprecated) from langchain.chat_models import ChatOpenAI # ConversationBufferMemory()ì˜ ì¸ìë¡œ ë“¤ì–´ê°ˆ llmìœ¼ë¡œ ChatOpenAIëª¨ë¸ì„ ì‚¬ìš©í•˜ê¸°ë¡œ í•¨
from langchain_community.chat_models import ChatOpenAI
from langchain.memory import ConversationBufferMemory # ëŒ€í™”ë‚´ìš©ì„ ì €ì¥í•˜ëŠ” memory
from langchain.chains import ConversationalRetrievalChain
# (deprecatd) from langchain.embeddings import OpenAIEmbeddings, HuggingFaceInstructEmbeddings
from langchain_community.embeddings import OpenAIEmbeddings, HuggingFaceInstructEmbeddings
# (deprecatd) from langchain.vectorstores import FAISS # ë¬¸ì„œê²€ìƒ‰ì„ ë‹´ë‹¹í•˜ëŠ” í˜ì´ìŠ¤ë¶ì´ ë§Œë“  ê³ ì† Vector DB. ë¡œì»¬ì— ì„¤ì¹˜í•˜ë©°, í”„ë¡œê·¸ë¨ ì¢…ë£Œì‹œ DBëŠ” ì‚­ì œë¨
from langchain_community.vectorstores import FAISS
from htmlTemplates import css, bot_template, user_template   # htmlTemplates.py íŒŒì¼ì•ˆì— ìˆëŠ” ëª¨ë“ˆë“¤ì„ ê°€ì ¸ì˜´

##############################
#        DB functions        #
##############################
# pdf ë¡œë”© : pdf_docs = st.file_uploader()
# ì´í›„ 3ë‹¨ê³„ : pdf_docs --(1)--> text --(2)-->  chunk --(3)--> vectorstore
# (1) .extract_texts
# (2) CTS
# (3) FAISS.from_texts

def get_pdf_text(pdf_docs):
    # ë¹ˆ text ë°°ì—´ ìƒì„±
    text = ""
    for pdf in pdf_docs:
        # í˜ì´ì§€ë³„ë¡œ ë°°ì—´ì„ ë¦¬í„´í•´ì£¼ëŠ” PdfReaderí´ë˜ìŠ¤ì˜ ê°ì²´ ìƒì„±
        pdf_reader = PdfReader(pdf)
        for page in pdf_reader.pages:
            # ê° í˜ì´ì§€ì˜ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì—¬ ëª¨ë“  textë¥¼ ë°°ì—´ë¡œ ì €ì¥ --> extract_text ë§¤ì†Œë“œë¥¼ ì´ìš©
            text += page.extract_text()
    return text

def get_text_chunk(text):
    text_splitter = CharacterTextSplitter(
        separator="\n",
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len
    )
    chunks = text_splitter.split_text(text)
    return chunks

def get_vectorstore(_text_chunks, _embeddings):
    vectorstore = FAISS.from_texts(_text_chunks, _embeddings)
    return vectorstore

######################################################
#  í•µì‹¬ í•¨ìˆ˜ GCC : get_conversation_chain(_vectorstore) #
######################################################
# chainê°ì²´ ìƒì„± í•¨ìˆ˜ : ConversationalRetrievalChain.from_llm() --> conversation_chain ê°ì²´ ë°˜í™˜
# chainê°ì²´ ìƒì„± 3ìš”ì†Œ(LLM, retriever, memory) --> ConversationalRetrievalChain.from_llm(chain ìƒì„± 3ìš”ì†Œ)
def get_conversation_chain(_vectorstore):
   # ë©”ëª¨ë¦¬ì— ë¡œë“œëœ envíŒŒì¼ì—ì„œ "OPENAI_API_KEY"ë¼ê³  ëª…ëª…ëœ ê°’ì„ ë³€ìˆ˜ë¡œ ì €ì¥
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    # ì„ íƒ 1: ëŒ€í™”ì— ì‚¬ìš©ë  llm API ê°ì²´ë¥¼ llm ë³€ìˆ˜ì— ì €ì¥
    llm = ChatOpenAI(
        temperature=0.1,    # ì°½ì˜ì„± (0.0 ~ 2.0)
        model_name="gpt-4-turbo-preview", # chatGPT-4 Turbo ì‚¬ìš©
        openai_api_key=OPENAI_API_KEY # Automatically inferred from env var OPENAI_API_KEY if not provided.
        )

    # ì„ íƒ 2: HuggingFaceHubë¥¼ llm ëª¨ë¸ë¡œ ì‚¬ìš©ì‹œ
    # from langchain.llms import HuggingFaceHub
    # llm = HuggingFaceHub(repo_id="google/flan-t5-xxl", model_kwargs={"temperature":0.5, "max_length":512})

    # ConverstaionBufferMemory í´ë˜ìŠ¤ë¥¼ ì´ìš©í•˜ì—¬ ëŒ€í™”ë‚´ìš©ì„ chat_historyë¼ëŠ” keyê°’ìœ¼ë¡œ ì €ì¥í•´ì£¼ëŠ” memory ê°ì²´ë¥¼ ìƒì„±
    memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

    # llm ê°ì²´, memory ê°ì²´ë¥¼ ì¸ìë¡œ ì…ë ¥í•˜ì—¬ DB ê²€ìƒ‰ê²°ê³¼ë¥¼ ì¶œë ¥í•˜ëŠ” ConversationalRetrievalChain.from_llm ê°ì²´ë¥¼ ìƒì„±
    conversation_chain = ConversationalRetrievalChain.from_llm(
        # ê²€ìƒ‰ì„ ì‹¤í–‰í•˜ëŠ” llm ì„ íƒ
        llm=llm,
        # ê²€ìƒ‰ì„ ë‹¹í•˜ëŠ” vector DBë¥¼ retriver í¬ë§·ìœ¼ë¡œ ì €ì¥
        retriever=_vectorstore.as_retriever(),
        # ì‚¬ìš©ìì™€ ëŒ€í™”ë‚´ìš©ì„ ë©”ëª¨ë¦¬ì— ì €ì¥í•˜ì—¬ ê°™ì€ ë§¥ë½ì—ì„œ ëŒ€í™”ë¥¼ ìœ ì§€
        memory=memory
    )
    # ConversationalRetrievalChain.from_llm ê°ì²´ë¡œ ìƒì„±ëœ convestaion_chainì„ ë°˜í™˜
    return conversation_chain

######################################################
#            í•µì‹¬ í•¨ìˆ˜ conversation_window             #
######################################################
# st.session_state.conversationì— GCCí•¨ìˆ˜ë¥¼ í†µí•´ ìƒì„±ëœ userì™€ì˜ ëŒ€í™”ê°ì²´ê°€ ì €ì¥ëœ ìƒíƒœ
# st.session_state.conversation ì—ì„œ ëŒ€í™”ë‚´ìš©ë§Œ ì¶”ì¶œ --> í”„ë¡ íŠ¸ì— ë¿Œë ¤ì¤Œ

def conversation_window(user_question) :
    # ConversationalRetrievalChain.from_llm() ì‹¤í–‰
    # --> conversation_chain ê°ì²´ ë°˜í™˜
    # --> st.sessioin_state.conversationì— ì €ì¥
    # ì§ˆë¬¸ --> ({'question': user_question}) í˜•íƒœë¡œ st.sessioin_state.conversation()ì— ì¸ìë¡œ ì…ë ¥ ë„£ì–´ì£¼ë©´ ê²°ê³¼ë¥¼ ì¶œë ¥í•˜ê³ , ëŒ€í™”ë‚´ìš©ì€ memoryì— ì €ì¥
    # ì§ˆë¬¸+ë‹µë³€ ì „ì²´ì´ë ¥ì„ ë³„ë„ì €ì¥ --> response['chat_history']ì— ì €ì¥

    ##################
    #     ì§ˆì˜ì‘ë‹µ     #
    ##################
    # main() í•¨ìˆ˜ ë§¨ë§ˆì§€ë§‰ì— st.session_state.conversation = get_conversation_chain(vectorstore) ì— ì˜í•˜ì—¬
    # st.session_state.conversationì—ëŠ” 'ì§ˆì˜ì‘ë‹µ'ì´ ì•„ë‹ˆë¼ conversation_chain 'í•¨ìˆ˜' ê·¸ ìì²´ê°€ ì €ì¥ë˜ì–´ ìˆìŒ
    # ë”°ë¼ì„œ conversation_chain() = ConversationalRetrievalChain.from_llm() ì´ë¯€ë¡œ
    # conversation_chain()ì—ì„œ ()ì•ˆì— {'question': user_question} í˜•íƒœë¡œ ì¸ìë¥¼ ë„£ì–´ì„œ ì§ˆë¬¸
    response = st.session_state.conversation({'question': user_question})

    # ConversationalRetrievalChain.from_llm()ëŠ” ì‘ë‹µì„ "ê°ì²´ë¡œ ë°˜í™˜"í•¨.
    # ë”°ë¼ì„œ response ì•ˆì—ëŠ” ì‘ë‹µê°ì²´ê°€ ì €ì¥ë˜ì–´ ìˆìœ¼ë©°, "ê°ì²´ì˜ keyê°’ì´ chat_history"ì— ëŒ€ì‘ë˜ëŠ” valueë¡œì„œ ì§ˆì˜/ì‘ë‹µì´ ì €ì¥ë¨.
    # í™•ì¸ --> st.write(response) í•´ë³´ë©´ chat_historyë¼ëŠ” keyê°’ì— ì§ˆì˜/ì‘ë‹µì´ ì €ì¥ë˜ì–´ ìˆìŒì„ ì•Œìˆ˜ìˆë‹¤.

    ################################
    #  ì§ˆì˜ì‘ë‹µ ëˆ„ì ì €ì¥ --> í”„ë¡ íŠ¸ ê²Œì‹œ  #
    ################################
    # ì‘ë‹µê°ì²´ì—ì„œ 'chat_history'ë§Œì„ ì¶”ì¶œí•œ í›„, st.session_stateì—ì„œ ë³„ë„ë¡œ ëˆ„ì ì ìœ¼ë¡œ ë³´ê´€í•˜ì—¬ ì „ì²´ ëŒ€í™”ë¥¼ ê¸°ë¡í•¨
    st.session_state.chat_history = response['chat_history']

    # message ê°ì²´ì˜ content ì†ì„±ì— ëŒ€í™”ê°€ ë“¤ì–´ìˆìœ¼ë¯€ë¡œ ì´ë¥¼ ì¶”ì¶œí•˜ì—¬ íƒ¬í”Œë¦¿ì˜ {{MSG}} ìœ„ì¹˜ì— ë„£ëŠ” replace ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ëŒ€ì²´
    for i, message in enumerate(st.session_state.chat_history):

        # 0ë¶€í„° ì‹œì‘í•˜ë¯€ë¡œ ì‚¬ìš©ì ì§ˆì˜ëŠ” í•­ìƒ ì§ìˆ˜ë²ˆì§¸ ê¸°ë¡
        if i % 2 == 0:
            st.write(user_template.replace(
                "{{MSG}}", message.content), unsafe_allow_html=True)

        # botì˜ ì‘ë‹µì€ í•­ìƒ í™€ìˆ˜ë²ˆì§¸ ê¸°ë¡
        else:
            st.write(bot_template.replace(
                "{{MSG}}", message.content), unsafe_allow_html=True)

######################################################
#                    admin key ê²€ì¦                   #
######################################################

def is_admin(_input_key):
    # ë¬¸ìì—´ì„ byteì—´ë¡œ encodingì„ ë¨¼ì € ì‹¤ì‹œí•œ í›„, sha256ìœ¼ë¡œ ì•”í˜¸í™”
    input_key_hash = hashlib.sha256(_input_key.encode()).hexdigest()
    saved_key_hash = hashlib.sha256(os.getenv("OPENAI_API_KEY").encode()).hexdigest()
    if input_key_hash == saved_key_hash :
        return True
    else :
        return False

###############################
#          (ì°¸ê³ )  ì±„íŒ…ì°½        #
###############################
# ì‚¬ì „ì— ì •ì˜í•œ css, htmlì–‘ì‹ì„ st.write() í•¨ìˆ˜ì˜ ì¸ìë¡œ ë„£ì–´ì£¼ë©´ ì›¹ì‚¬ì´íŠ¸ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•œë‹¤.
# st.write(user_template.replace("{{MSG}}", "Hellow Bot"), unsafe_allow_html=True)
# st.write(bot_template.replace("{{MSG}}", "Hellow Human"), unsafe_allow_html=True)



######################################################
#                        Main                        #
######################################################

def main() :
    load_dotenv()
    st.set_page_config(page_title="TONchat", page_icon=":books:", layout="wide")

    ##############################
    #       embeddings setup     #
    ##############################
    # embedding API ì„ íƒ
    # ì„ íƒ 1: OpenAI embedding API ì‚¬ìš©ì‹œ (ìœ ë£Œ)
    embeddings = OpenAIEmbeddings()

    # ì„ íƒ 2: í—ˆê¹…í˜ì´ìŠ¤ì—ì„œ ì œê³µí•˜ëŠ” Instructor embedding API ì‚¬ìš©ì‹œ (ë¬´ë£Œ)
    # ì„±ëŠ¥ì€ OpenAIEmbeddingsë³´ë‹¤ ìš°ìˆ˜í•˜ì§€ë§Œ ëŠë¦¬ë‹¤.
    # https://huggingface.co/hkunlp/instructor-xl
    # model_name ì¸ìê°’ìœ¼ë¡œ hkunlp/instructor-xlì„ ì…ë ¥
    # ë‹¨, 2ê°œì˜ dependencyë¥¼ ì„¤ì¹˜í•´ì•¼ í•¨ pip install instructorembedding sentence_transformers
    # embeddings = HuggingFaceInstructEmbeddings(model_name="hkunlp/instructor-xl")

    ##############################
    #      css, html setup       #
    ##############################
    st.write(css, unsafe_allow_html=True)

    ###############################
    #      DB initialization      #
    ###############################
    # FAISS.save_local ë©”ì†Œë“œëŠ” í´ë”ê°€ ì—†ìœ¼ë©´ error ì¶œë ¥ëŒ€ì‹  í•´ë‹¹í´ë”ë¥¼ ìƒì„±í•œë‹¤ëŠ” ì¥ì ì´ ìˆë‹¤
    # í•´ë‹¹í´ë”ì•ˆì— ê¸°ì¡´ì˜ index.faiss, index.pkl íŒŒì¼ì´ ìˆëŠ” ê²½ìš°ì—ëŠ” ë®ì–´ì“´ë‹¤
    vectorstore_dir = "vectorstore"
    vectorstore_file_path = 'vectorstore/index.faiss'
    if not os.path.exists(vectorstore_file_path):
        int_text = ["Tokamak Network offers customized L2 networks & simple way to deploy your own L2 based on your needs. Tokamak Network offers customized L2 networks & simple way to deploy your own L2 based on your needs"]
        vectorstore_init = FAISS.from_texts(int_text, embeddings)
        vectorstore_init.save_local(vectorstore_dir)

        if vectorstore_init is None:
            st.error("'vectorstore_init' is None")
            return

    ###############################
    #         DB loading          #
    ###############################
    vectorstore_loaded = FAISS.load_local(vectorstore_dir, embeddings)

    #######################################
    #  ì§ˆì˜/ì‘ë‹µ ê´€ë ¨ st.session_state ì´ˆê¸°í™”  #
    #######################################

    # main() í•¨ìˆ˜ ë§¨ ì•„ë˜ì— ìˆëŠ” st.session_state.conversation = get_conversation_chain(vectorstore)ì„ í†µí•´
    # session_state ê°ì²´ì˜ ì†ì„±ìœ¼ë¡œ conversation ì†ì„±ì´ ìƒì„±. ê·¸ ì•ˆì— ë”•ì…”ë„ˆë¦¬ë¡œ ì§ˆì˜/ì‘ë‹µì´ ì €ì¥ëœë‹¤.
    # { question : ddd, answer : ddd } ì´ëŸ°ì‹ì´ë‹¤.

    # ì‹¤í–‰ì— ì•ì„œ conversation ì´ˆê¸°í™”
    if "conversation" not in st.session_state:
        st.session_state.conversation = None

    # ì‹¤í–‰ì— ì•ì„œ chat_history ì´ˆê¸°í™”
    if 'chat_history' not in st.session_state:
        st.session_state.chat_history = None

    ###############################
    #            ì§ˆë¬¸ì°½             #
    ###############################
    # ì§ˆë¬¸ì…ë ¥ì°½
    st.header("TONchat")
    st.write("Ask a question about Tokamak Network's services")
    st.markdown('''
    - Titan L2 Network
      * Add Titan Network in Metamask
      * Developer Guide : (current) User Guide
      * Gas Estimation
      * How to Create a Standard ERC20 Token in L2
      * L2 fee
      * Titan-Goerli L2 Testnet Dev Document
      * Titan_User Guide
      * Token Address
      * What is different
    ''')
    st.subheader(":green[Enter your question]")
    # st.text_input()ì— ì§ˆë¬¸ì´ ì…ë ¥ë˜ë©´ Trueë¥¼ ë°˜í™˜
    user_question = st.text_input("", placeholder="ì˜ˆ) ì»¤ìŠ¤í…€ ERC 20 í† í°ì„ ìƒì„±í•˜ëŠ” ë°©ë²•ì„ ì•Œë ¤ë‹¬ë¼")

    # ì§ˆë¬¸ì´ ë“¤ì–´ì˜¤ë©´ ifë¬¸ì´ trueê°€ ë˜ê³ , ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì²˜ë¦¬í•œë‹¤.
    st.session_state.conversation = get_conversation_chain(vectorstore_loaded)
    if user_question:
        conversation_window(user_question)

    ###############################
    #      sidebar íŒŒì¼ ì—…ë¡œë“œ       #
    ###############################
    with st.sidebar:
        with st.popover("Admin login"):
            st.markdown("Admin key ğŸ”‘")

            # ì„¸ì…˜ ìƒíƒœì— admin ê°’ì´ ì—†ìœ¼ë©´ ì´ˆê¸°í™”
            if 'admin' not in st.session_state:
                st.session_state.admin = False

            # ì…ë ¥ í•„ë“œ ê°’ ë³€ê²½ ê°ì§€
            admin_key = st.text_input("Input your admin key")
            if st.button("Login"):
                st.session_state.admin = is_admin(admin_key)

        if st.session_state.admin:
            st.write("Hi, Admin !")
            if st.button("Logout", type='primary'):
                st.session_state.admin = False
                # ë¡œê·¸ì•„ì›ƒ í›„ ì¦‰ì‹œ ìŠ¤í¬ë¦½íŠ¸ ì¬ì‹¤í–‰(=page reload)
                st.experimental_rerun()

            st.header("DB setup", divider='rainbow')
            # upload multiple documents
            st.subheader("1. Add")
            pdf_docs = st.file_uploader("Only for updating DB, Upload PDFs and click on 'process'", accept_multiple_files=True)
            if st.button("Process") :
                with st.spinner('Processing') :

                    # DB ìƒì„± 3ë‹¨ê³„
                    ##########################
                    #     1. pdf --> text    #
                    ##########################
                    raw_text = get_pdf_text(pdf_docs)

                    ###########################
                    #    2. text --> chunks   #
                    ###########################
                    text_chunks = get_text_chunk(raw_text)
                    # st.write(text_chunks)

                    ###########################
                    # 3.chunk --> vectorstore #
                    ###########################
                    # create vectorstore_added
                    vectorstore_added = get_vectorstore(text_chunks, embeddings)
                    st.write(vectorstore_added)
                    if vectorstore_added is None:
                        st.error("Failed to create 'vectorstore_added'")
                        return

                    # merge with the existing DB
                    vectorstore_merged = vectorstore_added.merge_from(vectorstore_loaded)
                    if vectorstore_merged is None:
                        st.error("vectorstore_merged is None")
                        return

                    # save merged db permanently to the disk
                    vectorstore_merged.save_local(vectorstore_dir)
                    st.experimental_rerun()

                    #############################################################################
                    #    conversation chain ëŒ€í™” --> st.session_stateì— ê¸°ë¡ --> í”„ë¡ íŠ¸ì— ëŒ€í™” ì¶œë ¥    #
                    #############################################################################
                    # í•µì‹¬í•¨ìˆ˜ get_conversation_chain() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬, ì²«ì§¸, ì´ì „ ëŒ€í™”ë‚´ìš©ì„ ì½ì–´ë“¤ì´ê³ , ë‘˜ì§¸, ë‹¤ìŒ ëŒ€í™” ë‚´ìš©ì„ ë°˜í™˜í•  ìˆ˜ ìˆëŠ” ê°ì²´ë¥¼ ìƒì„±
                    # ë‹¤ë§Œ streamlit í™˜ê²½ì—ì„œëŠ” inputì´ ì¶”ê°€ë˜ê±°ë‚˜, ì‚¬ìš©ìê°€ ë²„íŠ¼ì„ ëˆ„ë¥´ê±°ë‚˜ í•˜ëŠ” ë“± ìƒˆë¡œìš´ ì´ë²¤íŠ¸ê°€ ìƒê¸°ë©´ ì½”ë“œ ì „ì²´ë¥¼ ë‹¤ì‹œ ì½ì–´ë“¤ì„
                    # ì´ ê³¼ì •ì—ì„œ ë³€ìˆ˜ê°€ ì „ë¶€ ì´ˆê¸°í™”ë¨.
                    # ë”°ë¼ì„œ ì´ëŸ¬í•œ ì´ˆê¸°í™” ë° ìƒì„±ì´ ë°˜ë³µë˜ë©´ ì•ˆë˜ê³  í•˜ë‚˜ì˜ ëŒ€í™” ì„¸ì…˜ìœ¼ë¡œ ê³ ì •í•´ì£¼ëŠ” st.sessiion_state ê°ì²´ì•ˆì— ëŒ€í™”ë¥¼ ì €ì¥í•´ì•¼ ë‚ ì•„ê°€ì§€ ì•ŠìŒ
                    # conversationì´ë¼ëŠ” ì†ì„±ì„ ì‹ ì„¤í•˜ê³  ê·¸ ì•ˆì— ëŒ€í™”ë‚´ìš©ì„ key, value ìŒìœ¼ë¡œ ì €ì¥ (ë”•ì…”ë„ˆë¦¬ ìë£Œí˜•)

                    vectorstore_loaded = FAISS.load_local(vectorstore_dir, embeddings)
                    st.session_state.conversation = get_conversation_chain(vectorstore_loaded)


            st.subheader("2. Delete")
            if st.button("Initialize DB"):
                # í•´ë‹¹ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
                if os.path.exists(vectorstore_dir):
                    # ë””ë ‰í† ë¦¬ì™€ ë‚´ìš©ë¬¼ ëª¨ë‘ ì‚­ì œ
                    shutil.rmtree(vectorstore_dir)
                    print(f"The directory '{vectorstore_dir}' has been deleted.")
                else:
                    print(f"The directory '{vectorstore_dir}' does not exist.")

                # DB ì—…ë°ì´íŠ¸í›„ í›„ ì¦‰ì‹œ ìŠ¤í¬ë¦½íŠ¸ ì¬ì‹¤í–‰(=page reload)
                st.experimental_rerun()


if __name__ == "__main__":
    main()
